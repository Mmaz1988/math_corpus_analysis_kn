{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b89cf35-aed9-4ed3-bd57-0cfe2a2c2cde",
   "metadata": {},
   "source": [
    "# Version 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55abc806-08ba-4dbe-9554-a12a5d64218d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import stanza\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from stanza.utils.conll import CoNLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02ce20f0-9c8c-41d5-b483-8016e4fd5e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chapter 1\n",
    "gr1 = 'data/gr1.tex'\n",
    "gr2 = 'data/gr2.tex'\n",
    "gr3 = 'data/gr3.tex'\n",
    "gr4 = 'data/gr4.tex'\n",
    "cas = 'data/cas.tex'\n",
    "leontief = 'data/leontief.tex'\n",
    "ppivot = 'data/ppivot.tex'\n",
    "network = 'data/network.tex'\n",
    "\n",
    "#Chapter 2\n",
    "vs1 = 'data/vs1.tex'\n",
    "vs2 = 'data/vs2.tex'\n",
    "vs3 = 'data/vs3.tex'\n",
    "fields = 'data/fields.tex'\n",
    "crystal = 'data/crystal.tex'\n",
    "voting = 'data/voting.tex'\n",
    "dimen = 'data/dimen.tex'\n",
    "\n",
    "#Chapter 3\n",
    "map1 = 'data/map1.tex'\n",
    "map2 = 'data/map2.tex'\n",
    "map3 = 'data/map3.tex'\n",
    "map4 = 'data/map4.tex'\n",
    "map5 = 'data/map5.tex'\n",
    "map6 = 'data/map6.tex'\n",
    "lstsqs = 'data/lstsqs.tex'\n",
    "homogeom = 'data/homogeom.tex'\n",
    "magicsqs = 'data/magicsqs.tex'\n",
    "markov = 'data/markov.tex'\n",
    "erlang = 'data/erlang.tex'\n",
    "\n",
    "#Chapter 4\n",
    "det1 = 'data/det1.tex'\n",
    "det2 = 'data/det2.tex'\n",
    "det3 = 'data/det3.tex'\n",
    "cramer = 'data/cramer.tex'\n",
    "detspeed = 'data/detspeed.tex'\n",
    "chio = 'data/chio.tex'\n",
    "projplane = 'data/projplane.tex'\n",
    "compgraphics = 'data/compgraphics.tex'\n",
    "\n",
    "#Chapter 5\n",
    "jc1 = 'data/jc1.tex'\n",
    "jc2 = 'data/jc2.tex'\n",
    "jc3 = 'data/jc3.tex'\n",
    "jc4 = 'data/jc4.tex'\n",
    "powers = 'data/powers.tex'\n",
    "pops = 'data/pops.tex'\n",
    "search = 'data/search.tex'\n",
    "recur = 'data/recur.tex'\n",
    "wilber = 'data/wilber.tex'\n",
    "innerproduct = 'data/innerproduct.tex'\n",
    "            #extras?\n",
    "eigengeom = 'data/eigengeom.tex'\n",
    "prinaxis = 'data/prinaxis.tex' #couldn't find it \n",
    "\n",
    "all_files =  [\n",
    "    gr1, gr2, gr3, gr4, cas, leontief, ppivot, network,\n",
    "    vs1, vs2, vs3, fields, crystal, voting, dimen,\n",
    "    map1, map2, map3, map4, map5, map6, lstsqs, homogeom, magicsqs, markov, erlang,\n",
    "    det1, det2, det3, cramer, detspeed, chio, projplane, compgraphics,\n",
    "    jc1, jc2, jc3, jc4, powers, pops, search, recur, wilber, innerproduct, eigengeom\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a540b886-c87e-4033-9586-f26e0c7c7956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_files(all_files, entire_content):\n",
    "    conc_txt = \"\"\n",
    "    for file_path in all_files:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                conc_txt += content + \"\\n\\n\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as output:\n",
    "        output.write(conc_txt)\n",
    "\n",
    "output_file = \"conc_txt.txt\"\n",
    "concatenate_files(all_files, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c123bb7d-f67b-433d-86f9-254f21fcfe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_path = 'conc_txt.txt'\n",
    "\n",
    "with open(output_file, 'r', encoding='utf-8') as file:\n",
    "    content = file.readlines()\n",
    "\n",
    "content_listed = list(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "508f386c-d23e-461e-b135-25acc7e65fbf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec6d4a37-b0d3-435f-b477-ad15a78f904b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove latex comments\n",
    "def clean_latex_to_text(latex_line):\n",
    "    cleaned_line = re.sub(r'%.*$', '', latex_line)  # remove LaTeX comments \n",
    "    cleaned_line = re.sub(r'\\\\in\\\\Re', ' in ‚Ñù', cleaned_line) # replace complex LaTeX commands like \\in\\Re\n",
    "    cleaned_line = re.sub(r'\\\\in', ' in', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\C', ' ‚ÑÇ', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\Re', ' ‚Ñù', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\RE', ' ‚Ñù', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\N', ' ‚Ñï', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\nbyn\\{(\\d+|n)\\}', r'N by N', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\nbym\\{([a-zA-Z\\d]+)\\}\\{([a-zA-Z\\d]+)\\}', r'N by M', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\nbyn\\{(\\d+|n)\\}', r'N by N', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\suchthat', ' such that ', cleaned_line) \n",
    "    cleaned_line = re.sub(r'\\\\vdotswithin\\{[^}]*\\}', '...', cleaned_line)  # Replace \\vdotswithin with \"...\" because it doesn't get applied in the next function\n",
    "    cleaned_line = re.sub(r'\\s+', ' ', cleaned_line).strip()  # normalize whitespace\n",
    "    return cleaned_line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "212cda4a-71dd-4c11-928c-6174e3c98044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multis(latex_line):\n",
    "    cleaned_line = re.sub(r'\\\\multicolumn\\{[^{}]*\\}\\{[^{}]*\\}\\{([^{}]*)\\}', r'\\1', latex_line)\n",
    "    cleaned_line = re.sub(r'\\\\multicolumn\\s*\\{[^}]*\\}\\s*\\{[^}]*\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\multiput\\s*\\([^)]*\\)\\s*\\([^)]*\\)\\s*\\{[^}]*\\}', '', cleaned_line)\n",
    "    \n",
    "    return cleaned_line\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5058043e-c726-4fb7-935b-e21bad642dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_math_symbols(latex_line):\n",
    "    \"\"\"Replaces mathematical symbols, Greek letters, and number sets.\"\"\"\n",
    "    math_symbols = {\n",
    "        r'\\\\neq': ' ‚â†', r'\\\\leq': '‚â§', r'\\\\geq': '‚â•', r'\\\\approx': '‚âà',\n",
    "        r'\\\\infty': '‚àû', r'\\\\forall': '‚àÄ', r'\\\\exists': '‚àÉ',\n",
    "        r'\\\\partial': '‚àÇ', r'\\\\nabla': '‚àá', r'\\\\times': '√ó',\n",
    "        r'\\\\div': '√∑', r'\\\\pm': '¬±', r'\\\\mp': '‚àì', r'\\\\sim': '‚àº',r'\\\\Dash' : '-',\n",
    "        r'\\\\mathbb{R}': '‚Ñù', r'\\\\mathbb{N}': '‚Ñï', r'\\\\mathbb{Z}': '‚Ñ§',\n",
    "        r'\\\\mathbb{Q}': '‚Ñö', r'\\\\mathbb{C}': '‚ÑÇ', r'\\\\sqrt': '‚àö', r'\\\\subseteq': '‚äÜ',\n",
    "        r'\\\\intersection': '‚à©',r'\\\\union': '‚à™',r'\\\\subsetneq': '‚ää', r'\\\\subset': '‚äÜ',\n",
    "        r'\\\\cap': '‚à©', r'\\\\cup': '‚à™',\n",
    "        r'\\\\sum': '‚àë', r'\\\\directsum': '‚äï', r'\\\\mathbb\\{B\\}': 'ùîπ',\n",
    "        r'\\\\F': 'ùîΩ', r'\\\\isomorphicto': '‚âÖ', r'\\\\mapsto': '‚Ü¶',\n",
    "        r'\\\\composed': '‚àò', r'\\\\dim': 'dim', r'\\\\supseteq': '‚äá',\n",
    "        r'\\\\perpv': '‚ä•'\n",
    "    }\n",
    "    greek_letters = {\n",
    "        r'\\\\alpha': 'Œ±', r'\\\\beta': 'Œ≤', r'\\\\gamma': 'Œ≥', r'\\\\delta': 'Œ¥',\n",
    "        r'\\\\epsilon': 'Œµ', r'\\\\zeta': 'Œ∂', r'\\\\eta': 'Œ∑', r'\\\\theta': 'Œ∏',\n",
    "        r'\\\\iota': 'Œπ', r'\\\\kappa': 'Œ∫', r'\\\\lambda': 'Œª', r'\\\\mu': 'Œº', ### change that for multicolumn\n",
    "        r'\\\\nu': 'ŒΩ', r'\\\\xi': 'Œæ', r'\\\\omicron': 'Œø', r'\\\\pi': 'œÄ',\n",
    "        r'\\\\rho': 'œÅ', r'\\\\sigma': 'œÉ', r'\\\\tau': 'œÑ', r'\\\\upsilon': 'œÖ',\n",
    "        r'\\\\phi': 'œÜ', r'\\\\chi': 'œá', r'\\\\psi': 'œà', r'\\\\omega': 'œâ',\n",
    "\n",
    "        r'\\\\varepsilon': 'Œµ', r'\\\\mathcal\\{E\\}': 'ùìî',r'\\\\mathcal\\{O\\}': 'ùí™',\n",
    "        r'\\\\mathscr\\{C\\}': 'ùíû',r'\\\\mathscr\\{N\\}': 'ùí©', r'\\\\mathscr\\{R\\}': '‚Ñõ',\n",
    "        r'\\\\mathcal\\{I\\}': 'ùìò'\n",
    "    }\n",
    "    dots = {\n",
    "        r'\\\\ldots': '...', r'\\\\dots': '...', r'\\\\cdot': '¬∑', r'\\\\vdotswithin{=}': '...', r'\\\\ddots' : '...',\n",
    "        r'\\\\vdots' : '...', r'\\\\alignedvdots': '...'\n",
    "        }\n",
    "    arrows = {\n",
    "        r'\\\\rightarrow': '->', r'\\\\leftarrow': '‚Üê', r'\\\\Rightarrow': '‚áí',\n",
    "        r'\\\\Leftarrow': '‚áê', r'\\\\uparrow': '‚Üë', r'\\\\downarrow': '‚Üì',\n",
    "        r'\\\\leftrightarrow': '‚Üî', r'\\\\Leftrightarrow': '‚áî', r'\\\\longrightarrow': '-->',\n",
    "        r'\\\\swap': ' ‚Üî ', r'\\\\iff': '‚áî', r'\\\\Longrightarrow': '-->'\n",
    "    }\n",
    "\n",
    "    stuff = {\n",
    "        r'\\\\qquad': '   ', r'\\\\quad': '   ', r'\\\\ell': 'l', r'\\\\arr': 'r',\n",
    "        r'\\\\zero': '0', r'\\\\text{and}': 'and', r'\\\\left' : '', r'\\\\right' : '',\n",
    "        r'\\\\suchthat' : 'such that', r'\\\\em': 'm', r'\\\\bigl': '[', r'\\\\bigr': ']',\n",
    "         r'\\\\big': '',  r'\\\\small': '', r'\\\\not': ' not ', r'\\\\implies': '‚áí'\n",
    "        \n",
    "    }\n",
    "    circled_numbers = {\n",
    "        '1': '‚ë†', '2': '‚ë°', '3': '‚ë¢', '4': '‚ë£', '5': '‚ë§',\n",
    "        '6': '‚ë•', '7': '‚ë¶', '8': '‚ëß', '9': '‚ë®', '10': '‚ë©',\n",
    "        '11': '‚ë™', '12': '‚ë´', '13': '‚ë¨', '14': '‚ë≠', '15': '‚ëÆ',\n",
    "        '16': '‚ëØ', '17': '‚ë∞', '18': '‚ë±', '19': '‚ë≤', '20': '‚ë≥'\n",
    "    }\n",
    "    \n",
    "    # replace \\digitincirc{X} with corresponding Unicode\n",
    "    def replace_digitincirc(match):\n",
    "        number = match.group(1)\n",
    "        return circled_numbers.get(number, number)\n",
    "\n",
    "    # replace \\digitincirc{X} with circled numbers\n",
    "    cleaned_line = re.sub(r'\\\\digitincirc\\{(\\d+)\\}', replace_digitincirc, latex_line)\n",
    "\n",
    "    \n",
    "    #cleaned_line = latex_line\n",
    "    for latex, uni_code in {**math_symbols, **greek_letters,**dots,**arrows,**stuff}.items():\n",
    "        cleaned_line = re.sub(latex, uni_code, cleaned_line)\n",
    "    return cleaned_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee986ef4-e2b1-459b-a944-f038afce7425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_matrix(latex_line):\n",
    "    cleaned_line = re.sub(r'\\\\begin{(mat|amat|vmat|pmat|pmatrix|smallmatrix|vmatrix)}(\\[[^\\]]*\\])?(\\{\\d+\\})?', '([', latex_line) # beggining of matrix represented as [[\n",
    "    cleaned_line = re.sub(r'\\\\end{mat}|\\\\end{amat}|\\\\end{vmat}|\\\\end{pmat}|\\\\end{pmatrix}|\\\\end{smallmatrix}|\\\\end{vmatrix}', '])', cleaned_line) #end of matrix represented as ]]\n",
    "    cleaned_line = re.sub(r'&', '  ', cleaned_line) # for all & we represent them as empty space\n",
    "    cleaned_line = re.sub(r'\\\\\\\\', '\\t', cleaned_line) # new lines in the matrices are changed to a new tab ## not enterirely representative here on jupyter\n",
    "\n",
    "    return cleaned_line.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f0e163a-b486-42c5-9996-e52d92a3ed4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def begs_and_ends(latex_line):\n",
    "    cleaned_line = re.sub(r'\\\\begin\\{equation\\*\\}', '$', latex_line)       #equations\n",
    "    cleaned_line = re.sub(r'\\\\end\\{equation\\*\\}', '$', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\begin{linsys}(\\[[^\\]]*\\])?(\\{\\d+\\})?', '$', cleaned_line)     #linsys\n",
    "    cleaned_line = re.sub(r'\\\\end\\{linsys}', '$', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\begin\\{align\\*\\}', '$', cleaned_line)      #align\n",
    "    cleaned_line = re.sub(r'\\\\end\\{align\\*\\}', '$', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\begin\\{align}', '$', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\end\\{aligned}', '$', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\begin\\{aligned}', '$', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\end\\{align}', '$', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\begin\\{example}', 'For example', cleaned_line)     #example\n",
    "    cleaned_line = re.sub(r'\\\\end\\{example}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\begin\\{definition}', '', cleaned_line)    #definition\n",
    "    cleaned_line = re.sub(r'\\\\end\\{definition}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\begin\\{proof}', '', cleaned_line)       #proof\n",
    "    cleaned_line = re.sub(r'\\\\end\\{proof}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\begin\\{lemma}', '', cleaned_line)    #lemma\n",
    "    cleaned_line = re.sub(r'\\\\end\\{lemma}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\begin\\{multiline\\*\\}', '$', cleaned_line)   #multiline\n",
    "    cleaned_line = re.sub(r'\\\\end\\{multiline\\*\\}', '$', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\begin\\{multline\\*\\}', '$', cleaned_line)   #multiline\n",
    "    cleaned_line = re.sub(r'\\\\end\\{multline\\*\\}', '$', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\begin\\{center}', '', cleaned_line)    #center\n",
    "    cleaned_line = re.sub(r'\\\\end\\{center}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\begin\\{exparts\\*\\}', '', cleaned_line)   #exparts\n",
    "    cleaned_line = re.sub(r'\\\\end\\{exparts\\*\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\begin\\{exparts}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\end\\{exparts}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\begin\\{answer}', '', cleaned_line)   #answer\n",
    "    cleaned_line = re.sub(r'\\\\end\\{answer}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\begin\\{exercises}', '', cleaned_line)   #exersice\n",
    "    cleaned_line = re.sub(r'\\\\end\\{exercises}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\begin\\{enumerate}', '', cleaned_line)   #enumerate\n",
    "    cleaned_line = re.sub(r'\\\\end\\{enumerate}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\begin{array}(\\{[a-zA-Z]\\})?', '$', cleaned_line)     #array\n",
    "    cleaned_line = re.sub(r'\\\\end\\{array}', '$', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\begin\\{theorem}', '', cleaned_line)   #theorem\n",
    "    cleaned_line = re.sub(r'\\\\end\\{theorem}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\begin\\{corollary}', '', cleaned_line)   #corollary\n",
    "    cleaned_line = re.sub(r'\\\\end\\{corollary}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\begin\\{input}\\s*', '', cleaned_line)   #input\n",
    "    cleaned_line = re.sub(r'\\\\end\\{input}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\begin\\{remark}', '', cleaned_line)   #remark\n",
    "    cleaned_line = re.sub(r'\\\\end\\{remark}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\begin\\{tabular}.*?', '', cleaned_line)   #tabular\n",
    "    cleaned_line = re.sub(r'\\\\end\\{tabular}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\begin\\{minipage}.*?', '', cleaned_line)   #minipage\n",
    "    cleaned_line = re.sub(r'\\\\end\\{minipage}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\begin\\{lstlisting}.*?', '', cleaned_line)   #lstlisting\n",
    "    cleaned_line = re.sub(r'\\\\end\\{lstlisting}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\begininput', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\endinput', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\begin\\{computercode\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\end\\{computercode\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\begin\\{itemize}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\end\\{itemize}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\begin\\{tfae}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\end\\{tfae}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\begin\\{aligncolondecimal\\}(\\{[^{}]*\\})?', 'aligned ', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\end\\{aligncolondecimal\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\begin\\{CD\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\end\\{CD\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\begin\\{strings\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\end\\{strings\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\begin\\{picture\\}\\([^)]*\\)', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\end\\{picture\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\begin\\{quotation\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\end\\{quotation\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\begin\\{split\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\end\\{split\\}', '', cleaned_line)\n",
    "    \n",
    "    \n",
    "\n",
    "    return cleaned_line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b89ebccf-8ec0-4aef-b8b9-0195518f07d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def steps(latex_line):\n",
    "    cleaned_line = re.sub(r'\\\\grstep(?:\\[[^\\]]*\\])?\\{([^{}]*)\\}', r'for step $\\1$', latex_line)\n",
    "    cleaned_line = re.sub(r'\\\\repeatedgrstep(?:\\[[^\\]]*\\])?\\{([^{}]*)\\}', r'repeat step $\\1$', cleaned_line)\n",
    "    \n",
    "    return cleaned_line\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "011f8ce4-183f-4ac2-8351-d5a3f1a6be1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_vecs(latex_line):\n",
    "    colvecs = re.findall(r'\\\\colvec(?:\\[[^\\]]*\\])?\\{(.*?)\\}', latex_line, re.DOTALL)\n",
    "    \n",
    "    for colvec in colvecs:\n",
    "        formatted_colvec = \"[\" + colvec.replace(\"\\\\\", \" \").replace(\"&\", \" \") + \"]\"  # Replace separators\n",
    "        pattern = r'\\\\colvec(?:\\[[^\\]]*\\])?\\{' + re.escape(colvec) + r'\\}'\n",
    "\n",
    "        #latex_line = re.sub(pattern, lambda m: re.escape(formatted_colvec), latex_line, flags=re.DOTALL)\n",
    "        latex_line = re.sub(r'\\\\colvec(?:\\[[^\\]]*\\])?\\{' + re.escape(colvec) + r'\\}', formatted_colvec, latex_line, flags=re.DOTALL) #new\n",
    "\n",
    "    return latex_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6c6a342-02a9-4747-8056-e5a194e89f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_vecs(latex_line):\n",
    "    rowvecs = re.findall(r'\\\\rowvec(?:\\[[^\\]]*\\])?\\{(.*?)\\}', latex_line, re.DOTALL)\n",
    "    \n",
    "    for rowvec in rowvecs:\n",
    "        formatted_rowvec = \"[\" + rowvec.replace(\"\\\\\", \" \").replace(\"&\", \" \") + \"]\" \n",
    "        pattern = r'\\\\rowvec(?:\\[[^\\]]*\\])?\\{' + re.escape(rowvec) + r'\\}'\n",
    "\n",
    "        latex_line = re.sub(r'\\\\rowvec(?:\\[[^\\]]*\\])?\\{' + re.escape(rowvec) + r'\\}', formatted_rowvec, latex_line, flags=re.DOTALL)\n",
    "\n",
    "    return latex_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5fa1efe-f2a8-42a6-9abe-a20227643988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_square_bracket_vectors(latex_line):\n",
    "    # replace \\[ and \\] with normal brackets\n",
    "    cleaned_line = re.sub(r'\\\\\\[(.*?)\\\\\\]', r'[\\1]', latex_line)\n",
    "    cleaned_line = re.sub(r'\\\\\\+', '+', cleaned_line)  # replace `\\+` with `+`\n",
    "    cleaned_line = re.sub(r'\\\\\\-', '-', cleaned_line)  # replace `\\-` with `-`\n",
    "    cleaned_line = re.sub(r'\\\\\\s+', ' ', cleaned_line)  # convert \"\\    \" to \" \"\n",
    "    cleaned_line = re.sub(r'\\\\\\t+', ' ', cleaned_line)  # convert \"\\t\" to a single space\n",
    "\n",
    "    # again the dots\n",
    "    cleaned_line = re.sub(r'\\\\dots|\\\\ldots|\\\\cdots|\\\\alignedvdots', '...', cleaned_line)\n",
    "\n",
    "    #remaining spacing inside the vector\n",
    "    cleaned_line = re.sub(r'\\s+', ' ', cleaned_line) \n",
    "\n",
    "    return cleaned_line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3583c77f-2d66-4bc2-8d0b-0638827d06ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_absval(latex_line):\n",
    "    cleaned_line = re.sub(r'\\\\absval\\{([^{}]+(\\{[^{}]*\\}[^{}]*)*)\\}', r'|\\1|', latex_line)\n",
    "  \n",
    "    return cleaned_line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff27f137-c80e-4883-8d0f-6c0bdae2e269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_latex_command(text, command, left_symbol, right_symbol):\n",
    "    \"\"\"Handles \\command{...} by replacing it with left_symbol...right_symbol while respecting nested {}.\"\"\"\n",
    "    stack = []\n",
    "    output = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(text):\n",
    "        if text[i:].startswith(f\"\\\\{command}{{\"):  # Found \\command{\n",
    "            stack.append(len(output))  # Remember where this command starts\n",
    "            output.append(left_symbol)  # Add the left symbol\n",
    "            i += len(f\"\\\\{command}\") + 1  # Move past the command name and opening {\n",
    "        elif text[i] == \"{\" and stack:  # Nested {\n",
    "            stack.append(len(output))\n",
    "            output.append(text[i])\n",
    "        elif text[i] == \"}\" and stack:  # Closing }\n",
    "            stack.pop()\n",
    "            if not stack:  # If this was the last closing brace for the command\n",
    "                output.append(right_symbol)  # Add the right symbol\n",
    "                i += 1  # Skip this closing }\n",
    "                continue\n",
    "\n",
    "        if i < len(text):\n",
    "            output.append(text[i])\n",
    "        i += 1 \n",
    "\n",
    "    return \"\".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ed3db12-b4b2-4318-9ba0-19d963988adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sets(latex_line):\n",
    "    cleaned_line = replace_latex_command(latex_line, \"set\", \"{\", \"}\")\n",
    "\n",
    "    return cleaned_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c969244-aa1f-4e78-bfba-663adc5c0ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removals(latex_line):\n",
    "    cleaned_line = re.sub(r'\\\\\\(', '$', latex_line)       #clean the dollars\n",
    "    cleaned_line = re.sub(r'\\\\\\)', '$', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\\\recommended\\s*', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\item\\s*', '', cleaned_line) # remove no content\n",
    "    cleaned_line = re.sub(r'\\\\section\\{[^{}]*\\}', '', cleaned_line) # remove with content\n",
    "    cleaned_line = re.sub(r'\\\\subsection\\{[^{}]*\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\label\\{[^{}]*\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\item\\{[^{}]*\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\partsitem\\{[^{}]*\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\partsitem\\s*', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\index\\{[^{}]*\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\noindent\\s*', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\index\\s*\\{.*?\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\nearbyexample\\{[^{}]*\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\nearbyexercise\\{[^{}]*\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\nearbytheorem\\{[^{}]*\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\nearbycorollary\\s*\\{[^{}]*\\}', '', cleaned_line) #lose info that werent interpretable meaning titles\n",
    "    cleaned_line = re.sub(r'\\\\nearbyremark\\s*\\{[^{}]*\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\nearbydefinition\\{([^{}]*)\\}', r'\\1', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\ref\\{[^{}]*\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\appendrefs\\{[^{}]*\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\includegraphics(?:\\[[^\\]]*\\])?\\{[^{}]*\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\recommended\\s*', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\nearbylemma\\{[^{}]*\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\tag\\{[^{}]*\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\cite\\{[^{}]*\\}', '', cleaned_line)\n",
    "    #new\n",
    "    cleaned_line = re.sub(r'\\\\spaceforemptycolumn\\s*', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\raisebox\\{[^{}]*\\}(\\[[^\\]]*\\])?(\\[[^\\]]*\\])?\\{[^{}]*\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\small\\s*', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\puzzle\\s*', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\hfill\\s*', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\dotproduct\\s*', 'dot product of ', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\dotprod\\s*', 'dot product of ', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\rm\\s*', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\answerasgiven\\s*', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\medskip\\s*', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\shortstack\\s*', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\subsectionoptional\\{[^{}]*\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\hline\\s*', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\tag\\*\\{[^{}]*\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\parbox\\{[^{}]*\\}\\{[^{}]*\\}', '', cleaned_line)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    cleaned_line = re.sub(r'\\\\chapter\\{[^{}]*\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\hbox\\{[^{}]*\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\topic\\{[^{}]*\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\answerasgiven\\{[^{}]*\\}', ' answer as given ', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\par\\{[^{}]*\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\hspace\\*?\\{[^{}]*\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\includegraphics(\\[[^\\]]*\\])?\\{[^{}]*\\}', '', cleaned_line)\n",
    "\n",
    "    cleaned_line = re.sub(r'\\\\lstinline(\\[[^\\]]*\\])?![^!]*?!', '', cleaned_line)\n",
    "\n",
    "    cleaned_line = re.sub(r'\\\\textbf\\{(.*?)\\}', r'\\1', cleaned_line, flags=re.DOTALL)\n",
    "    cleaned_line = re.sub(r'\\\\textit\\{(.*?)\\}', r'\\1', cleaned_line, flags=re.DOTALL)\n",
    "    cleaned_line = re.sub(r'\\\\text\\{(.*?)\\}', r'\\1', cleaned_line, flags=re.DOTALL)\n",
    "    cleaned_line = re.sub(r'\\\\spanof\\{(.*?)\\}', r'span(\\1)', cleaned_line, flags=re.DOTALL)\n",
    "    cleaned_line = re.sub(r'\\\\mbox\\{(.*?)\\}', r'\\1', cleaned_line, flags=re.DOTALL)\n",
    "\n",
    "\n",
    "    cleaned_line = re.sub(r'\\\\renewcommand\\{[^{}]*\\}\\{[^{}]*\\}', '', cleaned_line)\n",
    "\n",
    "    # Remove \\multicolumn but keep the inner content\n",
    "\n",
    "    cleaned_line = re.sub(r'@\\{[^{}]*\\}', '', cleaned_line)\n",
    "\n",
    "    cleaned_line = re.sub(r'\\\\cline\\{[^{}]*\\}', '', cleaned_line)\n",
    "\n",
    "    cleaned_line = re.sub(r'\\\\rule\\{[^{}]*\\}\\{[^{}]*\\}', '', cleaned_line)\n",
    "\n",
    "\n",
    "\n",
    "    cleaned_line = re.sub(r'\\\\map\\{([^\\{\\}]+)\\}\\{([^\\{\\}]+)\\}\\{([^\\{\\}]+)\\}', r'\\1: \\2 ‚Üí \\3', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\norm\\{([^{}]*)\\}', r'‚Äñ\\1‚Äñ', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\sequence(?:\\[[^\\]]*\\])?\\{([^{}]*)\\}', r'\\1', cleaned_line) ### \n",
    "    cleaned_line = re.sub(r'\\\\binom\\{([^{}]*)\\}\\{([^{}]*)\\}', r'(\\1 choose \\2)', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\lincombo\\{([^{}]*)\\}', r'Linear combination of \\1', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\rep\\{([^{}]*)\\}\\{([^{}]*)\\}', r'Representation of (\\1)', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\smash\\{([^{}]*)\\}', r'\\1', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\trans\\{([^{}]*)\\}', r'\\1·µÄ', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\rank\\{([^{}]*)\\}', r'rk(\\1)', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\hypertarget\\{[^{}]*\\}\\{([^{}]*)\\}', r'\\1', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\votinggraphic\\{([^{}]*)\\}', r'Voting graphic: \\1', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\votepreflist\\{([^{}]*)\\}\\{([^{}]*)\\}\\{([^{}]*)\\}', r'(\\1, \\2, \\3)', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\frac\\{([^{}]*)\\}\\{([^{}]*)\\}', r'(\\1/\\2)', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\mapsunder\\{([^{}]*)\\}', r'\\1 ‚Üì', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\rep\\{([^{}]*)\\}', r'\\1', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\rangespace\\{([^{}]*)\\}', r'\\1', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\generalmatrix\\{([^{}]*)\\}\\{([^{}]*)\\}\\{([^{}]*)\\}', r'general_matrix(\\1, \\2, \\3)', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\longmapsto', '‚Ü¶', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\overset\\{([^{}]*)\\}\\{([^{}]*)\\}', r'\\1 over \\2', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\underset\\{([^{}]*)\\}\\{([^{}]*)\\}', r'\\1 under \\2', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\highlight\\{([^{}]*)\\}', r'\\1', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\raisebox\\{[^{}]*\\}\\{[^{}]*\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\trace', 'tr', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\hline', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\identity', 'I', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\displaystyle', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\vcenteredhbox\\{[^{}]*\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\proj', 'proj', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\scriptstyle', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\composed\\{([^{}]+)\\}\\{([^{}]+)\\}', r'\\1 ‚àò \\2', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\xrightarrow\\{[^{}]*\\}', r'‚Üí', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\tiny', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\magicsquares_([a-zA-Z0-9]+)', r'MagicSquare_{\\1}', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\semimagicsquares_([a-zA-Z0-9]+)', r'SemiMagicSquare_{\\1}', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\makebox', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\colwidth\\]\\{\\$[^{}]*\\$\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\newcommand\\{\\\\[^{}]*\\}\\{\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\texttt\\{[^{}]*\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\section\\{[^{}]*\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\deter\\{([^{}]*)\\}', r'determinant(\\1)', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\substack\\{([^}]*)\\}', r'\\1', cleaned_line) \n",
    "    cleaned_line = re.sub(r'\\\\sgn\\{?([^{}]*)\\}?', r'sgn(\\1)', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\closedinterval\\{([^{}]*)\\}\\{([^{}]*)\\}', r'[\\1, \\2]', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\parbox\\{[^{}]*\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\adj\\{?([^{}]*)\\}?', r'adj(\\1)', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\hfil', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\mathbin', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\llap\\{[^{}]*\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\rangespace\\{([^{}]*)\\}', r'range(\\1)', cleaned_line) # spaces\n",
    "    cleaned_line = re.sub(r'\\\\gennullspace\\{([^{}]*)\\}', r'general nullspace(\\1)', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\genrangespace\\{([^{}]*)\\}', r'general rangespace(\\1)', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\put\\((-?\\d+),(-?\\d+)\\)\\{([^{}]*)\\}', r'At (\\1, \\2): \\3', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\rotatebox\\{-?\\d+\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\cat\\{([^{}]*)\\}', r'\\1', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\matrixvenlarge\\{([^{}]*)\\}', r'\\1', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\scriptsize\\s*', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\announcecomputercode\\s*', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\endinput\\s*', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\def\\\\smashdp#\\d+\\{[^\\}]*\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\smashdp\\{([^{}]*)\\}', r'\\1', cleaned_line)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    cleaned_line = re.sub(r'\\\\nbyn\\{([^{}]*)\\}', r'N by \\1', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\begin\\{subarray\\}\\{[^\\}]*\\}([^{}]*)\\\\end\\{subarray\\}', r'\\1', cleaned_line)\n",
    "\n",
    "    #lookabove\n",
    "    cleaned_line = re.sub(r'\\\\compconj\\{([^{}]*)\\}', r'\\1ÃÖ', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\hat\\{([^{}]*)\\}', r'\\1ÃÇ', cleaned_line)\n",
    "\n",
    "    #spaces\n",
    "    cleaned_line = re.sub(r'\\\\rowspace\\{([^}]*)\\}', r'rowspace \\1', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\?index\\s*\\{[^\\}]*\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\?includegraphics\\s*\\{[^\\}]*\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\?definend\\s*\\{([^{}]*)\\}', r'\\1', cleaned_line)\n",
    "    cleaned_line = re.sub(r'defbetweenrowvspace\\([^)]*\\)\\s*(\\[[^\\]]*\\])?\\s*(\\[[^\\]]*\\])?\\{[^}]*\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'definend\\s*\\{([^{}]*)\\}', r'\\1', cleaned_line)\n",
    "    cleaned_line = re.sub(r'textit\\s*\\{([^{}]*)\\}', r'\\1', cleaned_line)\n",
    "    cleaned_line = re.sub(r'mph\\s*\\{([^{}]*)\\}', r'\\1', cleaned_line)\n",
    "    cleaned_line = re.sub(r'subsection\\s*\\{([^{}]*)\\}', '', cleaned_line)\n",
    "\n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "    def clean_defind(match):\n",
    "        incontent = match.group(1)  # keep the inside for syntanctic- semantic purposes\n",
    "        clean_content = re.sub(r'[^a-zA-Z0-9 ]', '', incontent)\n",
    "        return clean_content \n",
    "\n",
    "    cleaned_line = re.sub(r'\\\\definend\\{([^{}]*)\\}', clean_defind, cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\/', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\\\.\\.\\.', '...', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\\\.', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\>\\s*', ' ', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\{@\\{.*?\\}@\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\{@\\{[^{}]*?\\}@\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\{[rlc|]+\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\!+', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\,', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\;', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'@>(.*?)>(.*?)>', r' ‚Üí (\\2) ', cleaned_line)  # for cds\n",
    "    cleaned_line = re.sub(r'@V\\{(.*?)\\}VV', r' ‚Üì (\\1) ', cleaned_line) \n",
    "    cleaned_line = re.sub(r'@A\\{(.*?)\\}AA', r' ‚Üë (\\1) ', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\\\$', '$', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\$\\s+\\$', '$', cleaned_line)\n",
    "\n",
    "    return cleaned_line.strip() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c58fdafd-a184-4781-bc4e-2a2758167a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_math_funcs(latex_line):\n",
    "    # agressive pattern\n",
    "    math_pattern = re.compile(r'\\\\([a-zA-Z]+)(?:\\^(\\d+))?\\s*(?:\\{([^{}]*)\\}|\\s*([a-zA-ZŒ±-œâŒë-Œ©0-9_Œ∏œÄ/+-]+))?')\n",
    "\n",
    "    def replace_math_func(match):\n",
    "        func_name = match.group(1)  # Function name: cos, sin, tan, etc.\n",
    "        exponent = match.group(2)  # Exponent (if present)\n",
    "        argument = match.group(3) if match.group(3) else match.group(4)  # Function argument\n",
    "\n",
    "        if not argument:\n",
    "            return func_name\n",
    "\n",
    "        # Format output: handle exponent cases\n",
    "        if exponent:\n",
    "            return f\"{func_name}‚Åø({argument})\".replace(\"‚Åø\", f\"^{exponent}\")\n",
    "        return f\"{func_name}({argument})\"\n",
    "\n",
    "    # apply\n",
    "    cleaned_line = re.sub(math_pattern, replace_math_func, latex_line)\n",
    "\n",
    "    return cleaned_line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f72cafb7-9c95-4a7f-b7e7-28131f2aef55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_graphics(latex_line):\n",
    "    cleaned_line = re.sub(r'\\\\?setlength\\s*\\{[^{}]*\\}\\s*\\{[^{}]*\\}', '', latex_line)\n",
    "    cleaned_line = re.sub(r'\\\\?put\\s*\\([^)]*\\)\\s*\\{[^{}]*\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\?put\\s*\\([^)]*\\)\\s*\\{\\s*\\}', '', cleaned_line) \n",
    "    cleaned_line = re.sub(r'\\\\?vector\\s*\\([^)]*\\)\\s*\\{[^{}]*\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\?line\\s*\\([^)]*\\)\\s*\\{[^{}]*\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\?raisebox\\s*\\([^)]*\\)\\s*(\\[[^\\]]*\\])?\\s*(\\[[^\\]]*\\])?\\s*\\{[^{}]*\\}', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\\\?vcenteredhbox\\s*\\([^)]*\\)', '', cleaned_line)\n",
    "    cleaned_line = re.sub(r'\\s+', ' ', cleaned_line).strip()\n",
    "    \n",
    "    return cleaned_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d1f9063-2baa-4c34-ba47-4d479305f11b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_latex_to_text(latex_line):\n",
    "    \"\"\"Main function to process LaTeX text while keeping $ for equations.\"\"\"\n",
    "    cleaned_line = clean_latex_to_text(latex_line)\n",
    "    cleaned_line = multis(cleaned_line)\n",
    "    cleaned_line = convert_math_symbols(cleaned_line)\n",
    "    cleaned_line = convert_matrix(cleaned_line)\n",
    "    cleaned_line = begs_and_ends(cleaned_line)\n",
    "    cleaned_line = steps(cleaned_line)\n",
    "    cleaned_line = row_vecs(cleaned_line)\n",
    "    cleaned_line = col_vecs(cleaned_line)\n",
    "    cleaned_line = convert_square_bracket_vectors(cleaned_line)\n",
    "    cleaned_line = convert_absval(cleaned_line)\n",
    "    cleaned_line = convert_sets(cleaned_line)\n",
    "    cleaned_line = removals(cleaned_line)\n",
    "    cleaned_line = convert_math_funcs(cleaned_line)\n",
    "    cleaned_line = remove_graphics(cleaned_line)\n",
    "    \n",
    "    \n",
    "    return cleaned_line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d6b20dc-12bb-4c1f-9810-f607ac35404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_content = [convert_latex_to_text(line) for line in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6dae11aa-3eb7-4b73-828f-560e2af7fd32",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pros_cont_str = ' '.join(processed_content)\n",
    "#print(pros_cont_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a94f5656-746c-4a0e-b0b0-491487cf50e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_file = \"lina_corpus.txt\"\n",
    "with open(final_output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(pros_cont_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d989ea-5ac3-4628-b2d8-43b5ee512310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70843241-e84e-459a-9904-17a3d22065a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e106ac7-ba27-48ed-810a-f73c927eda48",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "with open(final_output_file, \"r\") as f:\n",
    "    lina = f.read()\n",
    "\n",
    "nlp.max_length = 2000000  \n",
    "doc = nlp(lina)\n",
    "\n",
    "sentences = [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "sentence_output_file = \"lina_spacy_sentences.txt\"\n",
    "with open(sentence_output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b329443-7e35-463d-8a2f-daffdce56acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 111MB/s]\n",
      "2025-03-31 20:15:08 INFO: Downloaded file to /Users/soteresnkosdes/stanza_resources/resources.json\n",
      "2025-03-31 20:15:08 INFO: Downloading default packages for language: en (English) ...\n",
      "2025-03-31 20:15:09 INFO: File exists: /Users/soteresnkosdes/stanza_resources/en/default.zip\n",
      "2025-03-31 20:15:11 INFO: Finished downloading models and saved to /Users/soteresnkosdes/stanza_resources\n",
      "2025-03-31 20:15:11 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 34.7MB/s]\n",
      "2025-03-31 20:15:11 INFO: Downloaded file to /Users/soteresnkosdes/stanza_resources/resources.json\n",
      "2025-03-31 20:15:11 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-03-31 20:15:11 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| mwt       | combined |\n",
      "========================\n",
      "\n",
      "2025-03-31 20:15:11 WARNING: GPU requested, but is not available!\n",
      "2025-03-31 20:15:11 INFO: Using device: cpu\n",
      "2025-03-31 20:15:11 INFO: Loading: tokenize\n",
      "2025-03-31 20:15:12 INFO: Loading: mwt\n",
      "2025-03-31 20:15:12 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "stanza.download(\"en\")\n",
    "nlp = stanza.Pipeline(\"en\", processors=\"tokenize\", use_gpu=True)\n",
    "with open(\"lina_corpus.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lina = f.read()\n",
    "\n",
    "doc = nlp(lina)\n",
    "\n",
    "with open(\"lina_stanza_sentences.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for sentence in doc.sentences:\n",
    "        tokenized = \" \".join([word.text for word in sentence.words])\n",
    "        f.write(tokenized + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b442af3-d1fc-442f-8a26-82a175a45a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 35.5MB/s]\n",
      "2025-03-31 20:53:00 INFO: Downloaded file to /Users/soteresnkosdes/stanza_resources/resources.json\n",
      "2025-03-31 20:53:00 INFO: Downloading default packages for language: en (English) ...\n",
      "2025-03-31 20:53:01 INFO: File exists: /Users/soteresnkosdes/stanza_resources/en/default.zip\n",
      "2025-03-31 20:53:04 INFO: Finished downloading models and saved to /Users/soteresnkosdes/stanza_resources\n",
      "2025-03-31 20:53:04 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 37.3MB/s]\n",
      "2025-03-31 20:53:04 INFO: Downloaded file to /Users/soteresnkosdes/stanza_resources/resources.json\n",
      "2025-03-31 20:53:04 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-03-31 20:53:04 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2025-03-31 20:53:04 INFO: Using device: cpu\n",
      "2025-03-31 20:53:04 INFO: Loading: tokenize\n",
      "2025-03-31 20:53:04 INFO: Loading: mwt\n",
      "2025-03-31 20:53:04 INFO: Loading: pos\n",
      "2025-03-31 20:53:05 INFO: Loading: lemma\n",
      "2025-03-31 20:53:06 INFO: Loading: depparse\n",
      "2025-03-31 20:53:06 INFO: Done loading processors!\n",
      "Processing Sentences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10722/10722 [46:55<00:00,  3.81sent/s]\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if stanza.download('en') or torch.cuda.is_available() else \"cpu\"\n",
    "nlp = stanza.Pipeline(\n",
    "    \"en\", \n",
    "    processors=\"tokenize,pos,lemma,depparse\", \n",
    "    use_gpu=(device == \"cuda\")\n",
    ")\n",
    "\n",
    "sentence_file = \"lina_stanza_sentences.txt\"\n",
    "with open(sentence_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    sentences = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "\n",
    "all_sentences = []\n",
    "for line in tqdm(sentences, desc=\"Processing Sentences\", unit=\"sent\"):\n",
    "    doc = nlp(line)\n",
    "    all_sentences.extend(doc.sentences)\n",
    "\n",
    "\n",
    "if all_sentences:\n",
    "    doc = nlp(\"\")\n",
    "    doc.sentences = all_sentences\n",
    "\n",
    "    # Write to CoNLL-U\n",
    "    ud_output_file = \"lina_universal_dependencies.conllu\"\n",
    "    CoNLL.write_doc2conll(doc, ud_output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa294df0-5724-4d12-a93a-0fedd33b012e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for outputting csv \n",
    "\n",
    "#conllu_file_path = \"lina_universal_dependencies.conllu\"\n",
    "#csv_output_path = \"lina_ud_csv.csv\" \n",
    "\n",
    "\n",
    "#parsed_data = []\n",
    "#with open(conllu_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "#    for line in file:\n",
    "#        line = line.strip()\n",
    "#        if line.startswith(\"#\") or not line:\n",
    "#            continue\n",
    "#        fields = line.split(\"\\t\") \n",
    "#        if len(fields) == 10:  #conllu frame was 10\n",
    "#            parsed_data.append(fields)  \n",
    "\n",
    "## Dataframe\n",
    "#columns = [\"ID\", \"TEXT\", \"LEMMA\", \"UPOS\", \"XPOS\", \"FEATS\", \"HEAD\", \"DEPREL\", \"DEPS\", \"MISC\"]\n",
    "#df = pd.DataFrame(parsed_data, columns=columns)\n",
    "#df.to_csv(csv_output_path, index=False, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eca97e-04cb-47d4-8664-abd670d8aa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for outputting json\n",
    "\n",
    "#conllu_file_path = \"lina_universal_dependencies.conllu\"\n",
    "#json_output_path = \"lina_ud_json.json\" \n",
    "\n",
    "#sentences = []\n",
    "#current_sentence = []\n",
    "\n",
    "## Open and read the CoNLL-U file\n",
    "#with open(conllu_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "#    for line in file:\n",
    "#        line = line.strip()  # remove whitespace\n",
    "#        if line.startswith(\"#\"):  #ignore comments\n",
    "#            continue\n",
    "#        if not line:  # new sentence (empty line)\n",
    "#            if current_sentence:  # Append previous sentence\n",
    "#                sentences.append(current_sentence)\n",
    "#                current_sentence = []\n",
    "#            continue\n",
    "        \n",
    "#        fields = line.split(\"\\t\") \n",
    "#        if len(fields) == 10: \n",
    "#            token_data = {\n",
    "#                \"id\": fields[0],\n",
    "#                \"text\": fields[1],\n",
    "#                \"lemma\": fields[2],\n",
    "#                \"upos\": fields[3],\n",
    "#                \"xpos\": fields[4],\n",
    "#                \"feats\": fields[5],\n",
    "#                \"head\": fields[6],\n",
    "#                \"deprel\": fields[7],\n",
    "#                \"deps\": fields[8],\n",
    "#                \"misc\": fields[9]\n",
    "#            }\n",
    "#            current_sentence.append(token_data)\n",
    "\n",
    "\n",
    "#with open(json_output_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "#     json.dump(sentences, json_file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd650cf3-47dd-4d22-ad35-f7b216862a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_conllu_format(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    new_lines = []\n",
    "    current_sentence = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.rstrip(\"\\n\")\n",
    "        if line.strip() == \"\":\n",
    "            if current_sentence:\n",
    "                new_lines.extend(current_sentence)\n",
    "                new_lines.append(\"\")  # Add one blank line between sentences\n",
    "                current_sentence = []\n",
    "        else:\n",
    "            current_sentence.append(line)\n",
    "\n",
    "    # If something left in the last sentence\n",
    "    if current_sentence:\n",
    "        new_lines.extend(current_sentence)\n",
    "        new_lines.append(\"\")  # Final blank line\n",
    "\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(new_lines) + \"\\n\")  # final newline\n",
    "\n",
    "enforce_conllu_format(\"lina_universal_dependencies.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03a0086-8542-475d-9a35-cb2f0dfdca6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
